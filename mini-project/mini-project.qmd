---
title: "mini-project"
author: "Joshua Martin (PID: A18545389)"
format: pdf
toc: true
toc-depth: 2
number-sections: true
---
# Exploratory Data Analysis
## Background

The goal of this mini-project is for you to explore a complete analysis using the unsupervised learning techniques covered in class. You’ll extend what you’ve learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast masses. This expands on our RNA-Seq analysis from last day.

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

Values in this data set describe characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass.

# Data import

```{r, echo=TRUE, results='hide'}
read.csv("WisconsinCancer.csv")
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)
```

## Examine Data
```{r, echo=TRUE, results='hide'}
head(wisc.df)
```

## New data frame removing first row (diagnosis column)
```{r}
wisc.data <- wisc.df[,-1]
View(wisc.data)
head(wisc.df)
```

```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
View(diagnosis)
```

## Confirm Structures
```{r}
str(wisc.data)
table(diagnosis)
```

## Questions:
**Q1. How many observations are in this dataset?**
```{r}
dim(wisc.df)
```
```{r}
nrow(wisc.df)
```

There are `r nrow(wisc.data)` observations/patients in the dataset.

**Q2. How many of the observations have a malignant diagnosis?**
```{r}
table(diagnosis)
```

There are 212 malignant (M) and 357 benign (B) cases.

**Q3. How many variables/features in the data are suffixed with _mean?**
```{r}
length(grep("_mean$", colnames(wisc.data)))
```
There are 10 variables ending in _mean.

# Principal Component Analysis
The `prcomp()` function to do PCA has a `scale=FALSE` default. In general we always want to set this to TRUE so our analysis is not dominated by columns/variables in our dataset that have high standard deviation and mean when compared to others just because the units of measurement are on different scales.

# Check column means and standard deviations
```{r}
colnames(wisc.data)

apply(wisc.data,2,sd)
```

```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
```

```{r}
summary(wisc.pr)
```
The main PC result figure is called a "score plot" or "PC plot" or "ordination plot"...

```{r, echo=TRUE, results='hide'}
library(ggplot2)
wisc.pr$x
```

```{r}
library(ggplot2)

ggplot(wisc.pr$x) +
  aes(PC1,PC2, col=diagnosis) +
  geom_point()
```


## Questions
**Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?**
PC1 captures approximately 44.3% of the total variance in the dataset. 

**Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?**
At least 3 principal components (PC1-PC3) are needed to explain at least 70% of the variance.

**Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?**
At least 7 principal components (PC1-PC7) are needed to explain at least 90% of the variance.

## Create Biplot
```{r}
biplot(wisc.pr)
```
 **Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?**
 This plot is messy and difficult to analyze.
 
```{r}
plot(wisc.pr$x[, 1:2], col = diagnosis, 
     xlab = "PC1", ylab = "PC2")
```
**Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots**
```{r}
plot(wisc.pr$x[, c(1, 3)], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```
In each, there is strong clustering/less separation within the PC2 and PC3 groups, and strong separation along the PC1.

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col = diagnosis) + 
  geom_point()
```
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```
```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```
**Q9.For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?**
```{r}
wisc.pr$rotation["concave.points_mean", 1]
```

The loading of the concave.points_mean on PC1 is approximately `wisc.pr$rotation["concave.points_mean", 1]`.
So, higher values of concave.points_mean correspond to lower PC1 scores. PC1 separates malignant and benign cases, helping to distinguish them.

**Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?**
To explain at least 80% of the total variance, 4 principal components (PC1-PC$) is needed.

# Hierarchical clustering
```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

## Results of Hierarchical Clustering
```{r}
plot(wisc.hclust)
abline(h = 20, col = "red", lty = 2)

table(cutree(wisc.hclust,k=4))
```
This looks terrible.

**Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?**
The height at which 4 clusters occur is 20.

## Selecting number of clusters
```{r}
# Cut the dendrogram into 4 clusters
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)

# Compare cluster assignments to actual diagnoses
table(wisc.hclust.clusters, diagnosis)
```
```{r}
for (k in 2:10) {
  cat("\nNumber of clusters:", k, "\n")
  print(table(cutree(wisc.hclust, k = k), diagnosis))
}
```
**Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?**
Cutting the dendrogram into 4 clusters gives the best match to the true diagnoses. One cluster is mostly malignant, the other is mostly benign. Fewer clusters mix the two groups, and does not improve separation.

**Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.**
The ward.D2 method gives my preferred result. It creates clearer, interpretable groupings for this dataset.

# K-means Clustering
```{r}
# Create a k-means model with 2 clusters, scaled data, and 20 random starts
wisc.km <- kmeans(scale(wisc.data), centers = 2, nstart = 20)

# Compare k-means cluster membership to actual diagnoses
table(wisc.km$cluster, diagnosis)
```

**Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?**
K-means clustering separates the two diagnoses very well. Cluster 1 is mostly malignant and cluster 2 is mostly benign. K-means clustering is better at distinguishing clusters compared to hierarchical clustering.

```{r}
# Compare k-means clusters to hierarchical clustering clusters
table(wisc.hclust.clusters, wisc.km$cluster)
```
# Combining Methods

```{r}
# Use first 7 PCs (≥90% variance)
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")

# Visualize
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)

```

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")
```

```{r}
# Interactive 3D PCA plot (HTML only)
library(rgl)

# color by diagnosis 
grps <- ifelse(diagnosis == "M", "red", "blue")

plot3d(wisc.pr$x[, 1:3],
       xlab = "PC 1", ylab = "PC 2", zlab = "PC 3",
       cex = 1.5, size = 1, type = "s", col = grps)

rglwidget(width = 400, height = 400)  
```

```{r}
# Use the distance along the first 7 PCs for clustering
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")

# Cut into 2 clusters
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 2)

# Compare PCA-based hierarchical clusters to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```
**Q15. How well does the newly created model with four clusters separate out the two diagnoses?**
The PCA-based hierarchical clustering with 2 clusters separates the diagnoses well. Cluster 1 is mostly malignant, and cluster 2 is mostly benign.

**Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.**
K-means shows cleaner, more distinct clusters, while hierarchical clustering produces reasonable but less consistent grouping.

```{r}
# Compare k-means model clusters to actual diagnoses
table(wisc.km$cluster, diagnosis)

# Compare hierarchical clustering model clusters (before PCA) to actual diagnoses
table(wisc.hclust.clusters, diagnosis)
```
Clustering the original data was not very productive. The PCA results looked promising. Here we combine these methods by clustering from our PCA results. In other words "clustering in PC space"...

```{r}
## take the first 3 PCs
dist.pc <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(dist.pc, method="ward.D2")
```

View the tree...
```{r}
plot(wisc.pr.hclust)
abline(h=70, col="red")
```
To get our clustering membership vector (i.e. our main clustering result) we "cut" the tree at a desired height or to yield a desired number of "k" groups.

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

How does this clustering grps compare to the expert diagnosis

```{r}
table(grps, diagnosis)
```


## Sensitivity/Specificity
**Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?**

High Specificity: The PCA-based hierarchical clustering model corrently classifies most benign samples (329 B, 24 M) high TN rate / fewer false positives.

High Sensitivity: The k-means model captures the majority of malignant samples (175 M, 14 B) high TP rate / fewer false negatives.

# Prediction
We can use our PCA model for prediction with new input patient samples.

```{r}
# Load new data
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)

# Project new samples into the PCA space
npc <- predict(wisc.pr, newdata = new)

# Visualize
plot(wisc.pr$x[,1:2], col = g)             # original PCA points (colored by diagnosis)
points(npc[,1], npc[,2], col = "blue", pch = 16, cex = 3)  # new samples
text(npc[,1], npc[,2], c(1,2), col = "white")              # label samples
```
**Q18. Which of these new patients should we prioritize for follow up based on your results?**
Patient 2's sample pattern is more consistent with malignant characteristics and should be prioritized for clinical evaluation.

```{r}
sessionInfo()

```

